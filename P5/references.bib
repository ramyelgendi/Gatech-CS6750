@inbook{10.1145/3313831.3376442,
author = {Drosos, Ian and Barik, Titus and Guo, Philip J. and DeLine, Robert and Gulwani, Sumit},
title = {Wrex: A Unified Programming-by-Example Interaction for Synthesizing Readable Code for Data Scientists},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376442},
abstract = {Data wrangling is a difficult and time-consuming activity in computational notebooks,
and existing wrangling tools do not fit the exploratory workflow for data scientists
in these environments. We propose a unified interaction model based on programming-by-example
that generates readable code for a variety of useful data transformations, implemented
as a Jupyter notebook extension called Wrex. User study results demonstrate that data
scientists are significantly more effective and efficient at data wrangling with Wrex
over manual programming. Qualitative participant feedback indicates that Wrex was
useful and reduced barriers in having to recall or look up the usage of various data
transform functions. The synthesized code allowed data scientists to verify the intended
data transformation, increased their trust and confidence in Wrex, and fit seamlessly
within their cell-based notebook workflows. This work suggests that presenting readable
code to professional data scientists is an indispensable component of offering data
wrangling tools in notebooks.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12}
}

@inbook{10.1145/3313831.3376131,
author = {Xiao, Ziang and Zhou, Michelle X. and Chen, Wenxi and Yang, Huahai and Chi, Changyan},
title = {If I Hear You Correctly: Building and Evaluating Interview Chatbots with Active Listening Skills},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376131},
abstract = {Interview chatbots engage users in a text-based conversation to draw out their views
and opinions. It is, however, challenging to build effective interview chatbots that
can handle user free-text responses to open-ended questions and deliver engaging user
experience. As the first step, we are investigating the feasibility and effectiveness
of using publicly available, practical AI technologies to build effective interview
chatbots. To demonstrate feasibility, we built a prototype scoped to enable interview
chatbots with a subset of active listening skills-the abilities to comprehend a user's
input and respond properly. To evaluate the effectiveness of our prototype, we compared
the performance of interview chatbots with or without active listening skills on four
common interview topics in a live evaluation with 206 users. Our work presents practical
design implications for building effective interview chatbots, hybrid chatbot platforms,
and empathetic chatbots beyond interview tasks.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–14},
numpages = {14}
}

@inproceedings{10.1145/3434073.3444657,
author = {Das, Devleena and Banerjee, Siddhartha and Chernova, Sonia},
title = {Explainable AI for Robot Failures: Generating Explanations That Improve User Assistance in Fault Recovery},
year = {2021},
isbn = {9781450382892},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3434073.3444657},
doi = {10.1145/3434073.3444657},
abstract = {With the growing capabilities of intelligent systems, the integration of robots in
our everyday life is increasing. However, when interacting in such complex human environments,
the occasional failure of robotic systems is inevitable. The field of explainable
AI has sought to make complex-decision making systems more interpretable but most
existing techniques target domain experts. On the contrary, in many failure cases,
robots will require recovery assistance from non-expert users. In this work, we introduce
a new type of explanation, εerr, that explains the cause of an unexpected failure
during an agent's plan execution to non-experts. In order for error explanations to
be meaningful, we investigate what types of information within a set of hand-scripted
explanations are most helpful to non-experts for failure and solution identification.
Additionally, we investigate how such explanations can be autonomously generated,
extending an existing encoder-decoder model, and generalized across environments.
We investigate such questions in the context of a robot performing a pick-and-place
manipulation task in the home environment. Our results show that explanations capturing
the context of a failure and history of past actions, are the most effective for failure
and solution identification among non-experts. Furthermore, through a second user
evaluation, we verify that our model-generated explanations can generalize to an unseen
office environment, and are just as effective as the hand-scripted explanations.},
booktitle = {Proceedings of the 2021 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {351–360},
numpages = {10},
keywords = {fault recovery, explainable ai},
location = {Boulder, CO, USA},
series = {HRI '21}
}

@inproceedings{10.1145/3374920.3374924,
author = {Han, Changyo and Matsui, Katsufumi and Naemura, Takeshi},
title = {ForceStamps: Fiducial Markers for Pressure-Sensitive Touch Surfaces to Support Rapid Prototyping of Physical Control Interfaces},
year = {2020},
isbn = {9781450361071},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3374920.3374924},
doi = {10.1145/3374920.3374924},
abstract = {We present ForceStamps, fiducial markers for supporting rapid prototyping of physical
control interfaces on pressure-sensitive touch surfaces. We investigate marker design
options for supporting various physical controls, with focusing on creating dedicated
footprints and maintaining the structural stability. ForceStamps can be persistently
tracked on surfaces along with the force information and other attributes. Designers
without knowledge of electronics can rapidly prototype physical controls by attaching
mechanisms to ForceStamps, while manipulating the haptic feedback with buffer materials.
The created control widgets can be spatially configured on the touch surface to make
an interface layout. We showcase a variety of example controls created with ForceStamps.
In addition, we report on our analysis of a two-day musical instrument design workshop
to explore the affordances of ForceStamps for making novel instruments with diverse
interaction designs.},
booktitle = {Proceedings of the Fourteenth International Conference on Tangible, Embedded, and Embodied Interaction},
pages = {273–285},
numpages = {13},
keywords = {pressure sensing, tangible user interfaces, fiducial markers, physical controls},
location = {Sydney NSW, Australia},
series = {TEI '20}
}