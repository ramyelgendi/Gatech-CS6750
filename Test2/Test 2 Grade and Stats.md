Hey everyone! Test 2 is graded. I'm posting the entire explanation so that if you didn't read the explanation for Test 1, you don't have to go back and look it up. Test grades should be up in a couple hours.

**Test Grades**

For those of you that missed my earlier announcements, the test is effectively graded as 150 true/false questions. You get one point for each option you correctly mark, regardless of whether it's true or false. So, if the question was, Which of these are planets? A. Earth; B. Jupiter; C. The Moon; D. The Sun; E. Mercury; and you selected Earth, the Moon, and Mercury, you'd get 3 out of 5 points: 2 points for correctly marking Earth and Mercury, 1 point for correctly *not* marking The Sun. You'd lose a point for incorrectly not selecting Jupiter and for incorrectly selecting the Moon.

To access your test grade (when it's up in a couple hours), go to Grades in Canvas. Your grade is shown out of 150 points, corresponding to the 150 individual items on the exam, each of which could have been right or wrong.

**Test Stats**

Here are the test stats--note that we don't plan on a curve, so these stats are mostly just for your information, not for calculations like in some classes. That said, if we find that the changes to the tests this semester (and other changes, but mostly these) dramatically change the final grade distribution compared to previous semesters, we'll make adjustments accordingly. The beauty of gathering lots of data is that we can always reinterpret it if need be (and always such that individuals stand only to gain, not lose).

Here are the stats:

-   **Mean:** 127.65 (out of 150)

-   **Median:** 129.0

-   **SD:** 11.7

The distribution has a bit of a left skew. A few other interesting stats (well, interesting to me anyway):

-   The top score was 148 (1 person), followed by 147 (2 people) and 146 (2 people).

-   7 items were answered correctly by over 99% of the class.

-   44 items were answered correctly by over 95% of the class.

-   2 items were answered correctly by under 50% of the class.

-   9 items were answered correctly by under 60% of the class.

**Internal Validity Check**

Not all tests are created equal, though. We know there exist bad tests. How do you know if a test is good or bad? For us, we perform some validity checks to ensure that our test still represents a valid assessment. This is what gives us the confidence to use tests like these: it's often difficult to disentangle desirable difficulties from undesirable ones, but validity checks provide a strong way of doing that.

First, in a valid test, each individual item should be positively correlated with the overall score on the test--in other words, there should be no questions where people who got that problem *wrong* did better on the test overall than people who got that problem *right*. For this analysis, we treat each individual option as a single true/false question, and compare the averages of students who got that question right to averages of students who got that question wrong. Of course, under this procedure, there are 150 individual items, so I would expect a few questions to fail this check simply by random chance. I make the rule that if up to 5 questions failed this check, I keep them--if more than 4 failed, I throw out the questions with the biggest gaps. This is loosely based around statistical significance--I say each question has a 5% chance of being a "false positive" for having a poor correlation (e.g. it's a good question, but random chance this time makes it look poor), so out of 150 questions, we might expect up to 5 questions to generate false positives. (A more rigorous analysis would probably allow a couple more questions, but this should be fine for us.) As we offer this test several times, this also takes on some historical significance as well: if an item is invalid one semester but has been valid every previous semester, we'll keep it as well as it's far more likely for it to randomly fail once than to randomly pass several times. Similarly, if an item has failed repeatedly but has been one of less than 5 to fail, we'll still throw it out because it's more likely that it's actually invalid than that it's randomly turning up invalid multiple times.

The result this term: 4 questions failed this validity check. That's within our acceptable range, and three of these have passed several previous semesters' validity checks. The fourth is always interesting: it's a prompt that fails the validity check *every* semester, and yet it is copied verbatim from one of the readings. I think it's just that it's so non-intuitive that increased familiarity with the material is a detriment. But it's also only 1 point out of 150, meaning it makes effectively no difference on its own. It's just always interesting to me.

Most every problem has significant discriminability (meaning that students who answer the problem correctly do better than the problem's value alone on the test as a whole). So, this check suggests an internally valid test. Woo!

**External Validity Check**

The internal validity check makes sure that each item correlates to performance on the test as a whole: that means all the items are measuring effectively the same thing, and none are in competition with one another. However, that does not mean that the results are externally valid. External validity means that the test actually measures what it's trying to measure.

To check this, we compare grades on the test specifically to grades on other assessments. This isn't a perfect check, of course--we use tests and other assignments specifically because they assess different kinds of knowledge and ability. If they measured *exactly* the same thing, there would be no reason to have both! However, we do expect there to be some general correlation here: if assignments and tests were completed uncorrelated, then we would suspect they're individually measuring different things, which isn't our goal (although it is the goal of some classes). This also just checks whether the test and assignments measure the *same* thing, not whether both measure the *right* thing. Still, it's less likely for *both* to measure the wrong thing the same way than for only one to measure the wrong thing.

First, compared to Exam 1, there is unsurprisingly a high correlation at 0.79 (you may need to log into Ed and/or Canvas to see this image):

![](https://static.us.edusercontent.com/files/mVoVdzMwm2eG5g8qG76Zqgg4)

Second, compared to Homeworks, the correlation is 0.33, which is about where we want it: not so high that it's essentially duplicative, but not so low that it's totally unrelated (you may need to log into Ed and/or Canvas to see this image):

![](https://static.us.edusercontent.com/files/5dqjLSjMA5nDZdtAv4VOgVGd)

**Test Feedback**

We do the analyses above for two reasons. First, they're good for any class like this to do. Second and more importantly for us, though, they're the only check on the test questions' validity and fairness because--and this is the bad news--we don't share answer-by-answer feedback on an individual student basis.

The challenge is that we do reuse tests semester-to-semester. The reason isn't laziness: the reason is that it gives us a consistent barometer to assess how different semesters are doing, and it gives us more data to conclude that our tests are valid measures of outcomes. Even if assignments stay the same, the qualitative and subjective nature of assignment grading makes them a weaker barometer--and they *will* change, so that makes them even less reliable. It's important we assess whether or not we're making things better semester to semester. So, for that reason, we can't post a complete answer key--if we do, it's bound to get out. Even if no one in this class shares them (and I'm sure no one would), Piazza's security is very easy to circumvent. I really wish this all wasn't the case: the statistics are interesting, and I'd be curious to hear what others observe, but the risk is too high.

But we do want y'all to get feedback wherever possible. Toward that end, you'll find attached to your exam a long feedback comment. Contained in that comment are two things: first a breakdown of how well you performed on items related to each chapter on the test, and second, written feedback specifically tailored to the questions you answered incorrectly. The goal of these two things is to help you use the results to go recap the material you might have been weak on, rather than just give you corrections to small misconceptions. After all, these test questions are a barometer for your understanding: they aren't the sacrosanct canon of course knowledge, but rather small probes into how well you understand the material. If you struggled in one lesson in particular, it's more important that you recap that lesson as a whole rather than finding out the individual little things you got wrong. (Note that you may have fewer bits of feedback than points lost; in some cases, one explanation can cover multiple answers.)

In addition, we also like to share feedback on the questions that the class as a whole answers most poorly. We think this is a useful way of letting you know what others similarly struggled with. For this, we typically select the 10 most-incorrectly answered items. We ask that you do not share these outside the class as, even though they are intentionally obfuscated from the original questions.

1.  37.8% correct: According to Polson, Lewis, Rieman, and Wharton in Cognitive Walkthroughs, "Typical design guidelines ... provide very general recommendations that are not specific enough to guide the development process", "guidelines cannot handle complex interactions and trade-offs among design features", and "Others are distillations of experience with interface technology that is now out-of-date."

2.  41.3% correct: Primary stakeholders interact with the interface; secondary stakeholders may interact with the output of an interface, but not the interface itself; tertiary stakeholders typically are impacted by the interface's existence without ever using it or its output.

3.  53.5% correct: Between-subjects experiments have no ordering effects because participants each only experience one treatment.

4.  54.9% correct: According to Polson, Lewis, Rieman, and Wharton, "A cognitive walkthrough focuses on the interface being analysed, with questions designed to relate difficulties in mental processing to specific features of the interface, while PUM analysis focuses on the user's knowledge and reasoning. "

5.  55.4% correct: According to Cowan, changes in the American workforce did not lead to a significant increase in divoce rates.

6.  56.5% correct: A control variable is a variable that is controlled for all conditions in the experiment.

7.  58.7% correct: According to Cowan, the expanding use of technology in the home from 1920 to 1970 did not lower the time housewives spent on housework, but rather changed the expected outcomes.

8.  59.5% correct: Design-based research is a specific research methodology that uses designs to simultaneously investigate and solve a problem.

9.  59.8% correct: Although prototypes can be of varying levels of fidelity, the task itself should be representative of how the interface will really be used.

10. 61.7% correct: Participatory design invites users to come work with designers, while action research encourages practitioners to begin to act like researchers.

The reason we still feel confident in those questions, especially those with low percentages of correctness, is specifically the validity check above: although many of those questions were answered wrong by large portions of the class, the people who answered them correctly did better on the test as a whole--and in many cases, *far* better, which suggests those are actually specifically the best questions. Even if some questions are a bit tricky, that trickiness is helping differentiate students based on their overall performance, not randomness or misleading wording.

A couple other interesting stats: True statements were correctly marked 85.3% of the time, while False statements were correctly left unmarked 84.9% of the time. The lesson with the highest scores overall was 2.10 with an average score of 94.8%, and the lesson (well, section/topic) with the worst scores was Week 11 Readings with an average score of 72.8%.

Grades and feedback should be up in a couple hours!